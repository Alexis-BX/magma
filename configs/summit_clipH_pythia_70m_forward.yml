{
    # image encoder settings
    encoder_name: 'openclip-H',
    adapter_config: {"mlp": {"adapter_type": "normal", "downsample_factor": 8}, "attention": {"adapter_type": "normal", "downsample_factor": 8}},
    freeze_img_encoder: false,

    # language model settings
    lm_name: "neox",
    #lm_path: "EleutherAI/pythia-70m-deduped",
    lm_path: "EleutherAI/pythia-160m-deduped-v0",

    # train settings
    batch_size: 3,
    train_steps: 150000,
    lr: 8.0e-4,
    min_lr: 0.0,
    lr_decay_iters: 300000,
    image_enc_lr: 2.0e-6,
    use_image_embed_layernorm: true,
    image_embed_dropout_prob: 0.1,
    image_size: 384,

    gradient_accumulation_steps: 4,
    zero_stage: 2,
    gradient_clipping: 1.0,

    # dataset / save / load settings
    # dataset_type: 'new',
    # train_data: "/gpfs/alpine/csc499/proj-shared/LAION-400m-webdataset/data/{00000..37309}.tar",
    # val_data: "/gpfs/alpine/csc499/proj-shared/LAION-400m-webdataset/data/{37309..41455}.tar",
    # train_num_samples: 366598870,
    # train_num_samples: 407332084,

    eval_dataset_dir: null, # if this is none, train dataset will be split
    # vqa_dir: "/mnt/localdisk/vqa_val_converted",
    # gqa_dir: "/mnt/localdisk/gqa_val_converted",

    save_every: 2500,
    save: "/gpfs/alpine/scratch/alexisroger/csc499/magma/checkpoints/MAGMA_70M_clipH_1",
    load: "/gpfs/alpine/scratch/alexisroger/csc499/magma/checkpoints/MAGMA_70M_clipH_1",

    eval_every: 2500,
    wandb_project: "MAGMA_70M_clipH",
    name: "MAGMA_70M_clipH"
}
